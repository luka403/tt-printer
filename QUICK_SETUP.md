# ğŸš€ Quick Setup Guide - Remote LLM API

## ğŸ“‹ Koraci na Serveru

### 1. Instaliraj Ollama
```bash
curl -fsSL https://ollama.com/install.sh | sh
ollama pull llama3.1:8b
sudo systemctl enable ollama
sudo systemctl start ollama
```

### 2. Expose API (izaberi jednu opciju)

**OPCIJA A: Direktno (brzo)**
```bash
# U ~/.ollama/config ili environment
OLLAMA_HOST=0.0.0.0:11434
sudo systemctl restart ollama
sudo ufw allow 11434/tcp
```

**OPCIJA B: Nginx Reverse Proxy (preporuÄeno)**
- Vidi `SERVER_SETUP.md` za detaljna uputstva
- Kreiraj `/etc/nginx/sites-available/ollama-api`
- Enable i restart Nginx

### 3. Testiraj
```bash
curl http://TVOJ-SERVER-IP/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model":"llama3.1:8b","messages":[{"role":"user","content":"Hello"}]}'
```

---

## ğŸ“ Kada ZavrÅ¡iÅ¡, Javi Mi:

1. **API URL:** `http://IP/v1` ili `https://domen.com/v1`
2. **Model:** `llama3.1:8b` (ili koji koristiÅ¡)
3. **Autentifikacija:** Da li imaÅ¡ API key ili Basic Auth?

---

## ğŸ”§ AÅ¾uriranje Koda (Kada Mi KaÅ¾eÅ¡ URL)

### 1. AÅ¾uriraj `.env`:
```env
LLM_API_URL=http://TVOJ-SERVER-IP/v1
LLM_MODEL=llama3.1:8b
LLM_API_KEY=ollama  # Ili tvoj API key ako imaÅ¡ auth
```

### 2. Testiraj:
```bash
npm run test:llm
```

### 3. Pokreni pipeline:
```bash
npm start
```

---

## âœ… Sve je Spremno!

Tvoj kod veÄ‡ podrÅ¾ava remote API. Samo promeni URL u `.env` i radi! ğŸ‰










