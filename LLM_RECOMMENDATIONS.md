# ğŸ§  Najbolji Lokalni LLM Modeli za 2025 (16GB RAM)

## ğŸ† TOP 3 PREPORUKE za TikTok Skripte i PriÄe

### 1. **Llama 3.1 8B** â­â­â­â­â­ (NAJBOLJI IZBOR)
**ZaÅ¡to:**
- âœ… OdliÄan za kreativno pisanje (priÄe, skripte)
- âœ… Brz na CPU-u (8 vCPU Ä‡e ga pokrenuti za 5-15 sekundi po generaciji)
- âœ… Stanje: ~5GB RAM (q4_k_m quantization)
- âœ… PodrÅ¾ava dobar ton i stil (moÅ¾e da bude mraÄan, motivacioni, itd.)
- âœ… Lako se instalira preko Ollama

**Instalacija:**
```bash
ollama pull llama3.1:8b
```

**Konfiguracija u .env:**
```env
LLM_MODEL=llama3.1:8b
```

---

### 2. **Qwen 2.5 7B** â­â­â­â­
**ZaÅ¡to:**
- âœ… OdliÄan za kratke forme (TikTok skripte su kratke!)
- âœ… Brz (~3-10 sekundi)
- âœ… Stanje: ~4.5GB RAM
- âœ… Dobar za razliÄite jezike (ako planiraÅ¡ multi-language)

**Instalacija:**
```bash
ollama pull qwen2.5:7b
```

---

### 3. **Mistral 7B** â­â­â­â­
**ZaÅ¡to:**
- âœ… Brz i efikasan
- âœ… Stanje: ~4GB RAM
- âœ… Dobar balans kvaliteta/brzine

**Instalacija:**
```bash
ollama pull mistral:7b
```

---

## ğŸš€ INSTALACIJA (Ollama)

### 1. Instaliraj Ollama na serveru:
```bash
# Linux
curl -fsSL https://ollama.com/install.sh | sh

# Ili preuzmi sa: https://ollama.com/download
```

### 2. Pokreni Ollama servis:
```bash
ollama serve
# Ili kao systemd service:
sudo systemctl enable ollama
sudo systemctl start ollama
```

### 3. Instaliraj model:
```bash
ollama pull llama3.1:8b
```

### 4. Testiraj:
```bash
ollama run llama3.1:8b "Write a scary two-sentence story about mirrors"
```

---

## ğŸ“Š PERFORMANSE (na tvom serveru - 8 vCPU, 16GB RAM)

| Model | RAM Usage | Generacija (CPU) | Kvalitet Skripti |
|-------|-----------|------------------|------------------|
| **Llama 3.1 8B** | ~5GB | 5-15s | â­â­â­â­â­ |
| Qwen 2.5 7B | ~4.5GB | 3-10s | â­â­â­â­ |
| Mistral 7B | ~4GB | 3-8s | â­â­â­â­ |
| Phi-3.5 3.8B | ~2.5GB | 2-5s | â­â­â­ |

---

## ğŸ¯ PREPORUKA ZA TVOJ SLUÄŒAJ

**Za Scary Stories / TikTok Skripte:**
â†’ **Llama 3.1 8B** je najbolji izbor jer:
- Najbolji kvalitet kreativnog pisanja
- Dobar balans brzine i kvaliteta
- Ima dovoljno "kreativnosti" za horor priÄe
- Brz dovoljno da ne blokira pipeline

**Za Production (kad skaliraÅ¡ na viÅ¡e kanala):**
- MoÅ¾eÅ¡ da koristiÅ¡ **Qwen 2.5 7B** za brÅ¾e generisanje
- Ili **Phi-3.5 3.8B** ako trebaÅ¡ da generiÅ¡eÅ¡ 10+ videa istovremeno

---

## âš™ï¸ OPTIMIZACIJA za tvoj server

### 1. Podesi num_threads u Ollama:
```bash
# U ~/.ollama/config ili environment variable
export OLLAMA_NUM_THREADS=8  # Koristi sve tvoje CPU jezgre
```

### 2. Koristi quantization q4_k_m (default):
- Najbolji balans kvaliteta/veliÄine
- q8 je bolji ali sporiji i veÄ‡i

### 3. Cache model u RAM-u:
```bash
# Ollama automatski keÅ¡ira model nakon prve upotrebe
# Prvi poziv Ä‡e biti sporiji (~30s), sledeÄ‡i brÅ¾i (~5-10s)
```

---

## ğŸ”§ AÅ½URIRANJE TVOG KODA

Tvoj kod veÄ‡ koristi Ollama format! Samo promeni `.env`:

```env
LLM_API_URL=http://localhost:11434/v1
LLM_MODEL=llama3.1:8b
LLM_API_KEY=ollama
```

I to je to! Sistem Ä‡e automatski koristiti novi model.

---

## ğŸ§ª TESTIRANJE

Nakon instalacije, testiraj:

```bash
# 1. Proveri da li Ollama radi
curl http://localhost:11434/api/tags

# 2. Testiraj model direktno
ollama run llama3.1:8b "Write a scary TikTok story in 2 sentences"

# 3. Testiraj kroz tvoj sistem
npx ts-node main.ts
```

---

## ğŸ’¡ BONUS: Alternativa (ako ne Å¾eliÅ¡ Ollama)

**LM Studio** (GUI) ili **llama.cpp** direktno:
- ViÅ¡e kontrole
- MoÅ¾eÅ¡ da koristiÅ¡ custom quantization
- Ali komplikovanije za setup

**Preporuka:** Koristi Ollama - najlakÅ¡e i najbrÅ¾e za poÄetak! ğŸš€










